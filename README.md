# Image Classification and Model Interpretability

The purpose of this project is to explain the decisions of a Convolutional Neural Network.
If we are trying to build a model to make important decisions, like in healthcare, we probably want to know the reasons why the model took its decisions. A good accuracy or recall is not guarantee that the model distinguish the classes in the right way. Thus, we will try to investigate what patterns have been learned by our model. 

This project is about detecting pneumonia or no pneumonia from x-ray images of pediatric patients and try to explain the predictions of the Network. The explanations from the model was compared to Medical Doctors' statements. 
